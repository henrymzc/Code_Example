{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/henryma/anaconda3/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/henryma/anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/henryma/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/henryma/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/henryma/anaconda3/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/henryma/anaconda3/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/henryma/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def scrape_page(url,headers):\n",
    "    # Step 1: Send a request to the webpage\n",
    "\n",
    "    response = requests.get(url,headers=headers)\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Step 2: Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "        # Step 3: Extract data\n",
    "        id = [p.text for p in soup.find_all(attrs={'class': 'cusField p archive_no'}) if p.text != '档号']\n",
    "        eid = [p.text for p in soup.find_all(attrs={'class': 'cusField p archive_eid'}) if p.text != '电子档号']\n",
    "        titles = [p.text for p in soup.find_all(attrs={'class': 'cusField p title'}) if p.text != '题名']\n",
    "        names_a = [p.text for p in soup.find_all(attrs={'class': 'cusField p official_name_a'}) if p.text != '责任者A']\n",
    "        names_b = [p.text for p in soup.find_all(attrs={'class': 'cusField p official_name_b'}) if p.text != '责任者B']\n",
    "        pos_a = [p.text for p in soup.find_all(attrs={'class': 'cusField p official_position_a'}) if p.text != '官职爵位A']\n",
    "        pos_b = [p.text for p in soup.find_all(attrs={'class': 'cusField p official_position_b'}) if p.text != '官职爵位B']\n",
    "        old_choro = [p.text for p in soup.find_all(attrs={'class': 'cusField p old_chronology'}) if p.text != '原纪年']\n",
    "        return id,eid,titles,names_a,names_b,pos_a,pos_b,old_choro\n",
    "    else:\n",
    "        print(f'Failed to retrieve the webpage. Status code: {response.status_code}')\n",
    "        return []\n",
    "base_url = 'https://fhac.com.cn/ess/catalogue.html?0.6440211435717598&tpl_file=search_catalogues&pagesize=50&category_type=catalogue1&is_filter=1&filters%5B1%5D%5Bfilter_rel%5D=0&filters%5B1%5D%5Bfilter_field%5D=title&filters%5B1%5D%5Bfilter_type%5D=0&filters%5B1%5D%5Bfilter_kw%5D=%E8%9D%97&filters%5B2%5D%5Bfilter_rel%5D=1&filters%5B2%5D%5Bfilter_field%5D=title&filters%5B2%5D%5Bfilter_type%5D=0&filters%5B2%5D%5Bfilter_kw%5D=%E8%9D%BB&catalog%5B%5D=03&catalog%5B%5D=04&p='\n",
    "num_pages = 43\n",
    "headers = {\n",
    "    'X-Requested-With': 'XMLHttpRequest'\n",
    "}\n",
    "\n",
    "all_id = []\n",
    "all_eid = []\n",
    "all_titles = []\n",
    "all_names_a =[]\n",
    "all_names_b =[]\n",
    "all_pos_a = []\n",
    "all_pos_b = []\n",
    "all_old_choro =[]\n",
    "\n",
    "for i in range(1,num_pages+1):\n",
    "    url = f'{base_url}{i}'\n",
    "    id,eid,titles,names_a,names_b,pos_a,pos_b,old_choro = scrape_page(url, headers)\n",
    "    all_id.extend(id)\n",
    "    all_eid.extend(eid)\n",
    "    all_titles.extend(titles)\n",
    "    all_names_a.extend(names_a)\n",
    "    all_pos_a.extend(pos_a)\n",
    "    all_names_b.extend(names_b)\n",
    "    all_pos_b.extend(pos_b)\n",
    "    all_old_choro.extend(old_choro)\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Create a DataFrame\n",
    "data = {\n",
    "    'id': all_id,\n",
    "    'eid': all_eid,\n",
    "    'title': all_titles,\n",
    "    'name_a': all_names_a,\n",
    "    'pos_a': all_pos_a,\n",
    "    'name_b': all_names_b,\n",
    "    'pos_b': all_pos_b,\n",
    "    'old_choro': all_old_choro\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "file_path = '/Users/henryma/Dropbox/Research/Locust_Disaster/Data/tiben_nan.xlsx'\n",
    "df.to_excel(file_path, index=False)\n",
    "print('Data has been written')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
